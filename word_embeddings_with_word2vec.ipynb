{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef84835",
   "metadata": {},
   "source": [
    "**Disclaimer**: This Jupyter Notebook contains content generated with the assistance of AI. While every effort has been made to review and validate the outputs, users should independently verify critical information before relying on it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05082f9d",
   "metadata": {},
   "source": [
    "# Word Embeddings and Word2Vec\n",
    "\n",
    "In this notebook, we introduce two fundamental ideas in modern Natural Language Processing:\n",
    "\n",
    "## 1. Word Embeddings\n",
    "These are techniques that represent words as continuous, dense vectors in a multi-dimensional space. The main advantage of word embeddings is that they capture the underlying semantic relationships between words, making it easier for machine learning algorithms to process and understand natural language.\n",
    "\n",
    "## 2. Word2Vec\n",
    "Word2Vec is a popular method for generating word embeddings using a shallow neural network. It comes in two primary flavors: Skip-gram and Continuous Bag of Words (CBOW). It models a word prediction task, and the weights that are optimized ultimately give us our embedding vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f001a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59934a9a",
   "metadata": {},
   "source": [
    "# Dense Word Embeddings: A Crash Course\n",
    "\n",
    "\n",
    "## Embeddings\n",
    "Imagine asking a computer, “How was your day?”, only to discover that it doesn’t understand a word because it hasn’t been taught English. \n",
    "\n",
    "While this is a playful example, it underscores a significant challenge: there is an inherent language barrier between human communication and machine understanding.\n",
    "\n",
    "Traditionally, we overcome this limitation by using structured programming languages—explicitly instructing the computer to map specific phrases to predetermined representations. However, natural human language does not conform to rigid rules. Thus, we require a representation that computers can both interpret and act upon effectively.\n",
    "\n",
    "\n",
    "This leads us to the concept of a numerical representation. If we can encode each word as a vector, the computer can then leverage the power of linear transformations to transform and manipulate these representations. (For an excellent introduction, check out <a href = \"https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\"> Essence of Linear Algebra by 3Blue1Brown</a>.)\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src = \"images/embeddings.png\">\n",
    "</div>\n",
    "\n",
    "Thus, we arrive at the idea of word embeddings -- a mechanism that translates natural language into vectors which the machine can process. However, assigning a unique vector for every word (known as a sparse embedding) quickly becomes impractical due to the enormous size of a typical vocabulary (imagine storing a million-dimensional vector for English!).\n",
    "\n",
    "## Dense Embeddings\n",
    "\n",
    "To address the massive size of sparse vectors, our goal is to condense word representations into a dense, lower-dimensional space. Mathematically, the problem is to learn a mapping\n",
    "\n",
    "  $$f: V \\to \\mathcal{R}^d$$\n",
    "\n",
    "where each word $w \\in V$ is associated with a dense vector $f(w) \\in \\mathcal{R}^d$, and the dimensionality d is much smaller than the size of the vocabulary, $|V|$. You can think of $V$ as a kind of word database\n",
    "\n",
    "For example, consider a simplified vocabulary \n",
    "$V = \\{\\text{Mouse, Lion, Whale, Fish, Shark}\\}$. \n",
    "\n",
    "While one could represent these words in a five-dimensional sparse space, a more efficient approach is to choose a space where the axes might capture features such as “Terrestrial” versus “Aquatic” or “Small” versus “Big.” In this setting, even a two-dimensional representation might suffice for distinguishing between these concepts effectively.\n",
    "\n",
    "\n",
    "<div align = \"center\">\n",
    "    <img src = \"images/embedding_small_example.png\" style=\"width:60%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "How do we generate these dense word embeddings? The answer: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfbf46",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ec5f2",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec is a technique in natural language processing that learns to represent words as dense vectors. At its heart, Word2Vec treats language as a prediction problem, where the goal is to learn which words are likely to appear together. By framing language this way, the model uncovers deep semantic and syntactic relationships among words.\n",
    "\n",
    "## Prediction Tasks: CBOW and Skip-Gram\n",
    "\n",
    "Word2Vec uses two fundamental architectures, each based on a different prediction task:\n",
    "\n",
    "- **Continuous Bag of Words (CBOW):**  \n",
    "  In CBOW, the model is given a set of context words (the words surrounding a target word) and is trained to predict the target word. Imagine seeing the sentence “The quick brown ___ jumps over the lazy dog” and trying to fill in the blank. By aggregating the context, the model learns to predict the missing word. This approach tends to be fast and works well with smaller datasets.\n",
    "\n",
    "- **Skip-Gram:**  \n",
    "  The Skip-Gram model flips the prediction task. Given a target word, it predicts the words that are likely to appear in its context. For example, if the word “bank” is given, the model learns to predict surrounding words like “money”, “credit”, or even “river,” depending on the context. Skip-Gram is particularly effective at learning representations for rare words and performs better with larger datasets.\n",
    "\n",
    "<div align = \"center\">\n",
    "    <img src = \"images/skipgram_cbow_illustration.png\" style=\"width:60%;\">\n",
    "    <figcaption> Source: https://medium.com/data-science/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314 </figcaption>\n",
    "</div>\n",
    "\n",
    "## Architecture and Optimization\n",
    "\n",
    "We use a simple two-layer neural network to perform this prediction:\n",
    "\n",
    "1. **Input Layer:**  \n",
    "   Each word is initially represented as a one-hot encoded vector. This representation, however, is not used directly but rather serves as an index into the embedding matrix.\n",
    "\n",
    "2. **Embedding (Projection) Layer:**  \n",
    "   The core of the model is the embedding layer, where each word’s one-hot vector is mapped to a dense, lower-dimensional vector. These vectors are the word embeddings that encode the semantic properties of the words.\n",
    "\n",
    "3. **Output Layer:**  \n",
    "   Depending on the architecture, the output layer predicts either the target word (for CBOW) or the context words (for Skip-Gram). The model is trained to maximize the probability of these predictions.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src = \"images/word2vec_neural_network.png\" style=\"width:100%;\">\n",
    "    <figcaption> Source: https://link.springer.com/article/10.1007/s10462-023-10466-8 </figcaption>\n",
    "</div>\n",
    "\n",
    "## How Does Word2Vec Generate Embeddings?\n",
    "\n",
    "During training, the model continually adjusts the embedding weights in order to minimize a loss function, typically cross-entropy, using gradient descent. This iterative process gradually shifts the representations so that words appearing in similar contexts are placed closer together in the vector space. At the end of training, our weights represent our embedding vectors.\n",
    "\n",
    "## The Math behind Word2Vec\n",
    "\n",
    "<a href = \"https://stopwolf.github.io/blog/nlp/paper/2020/08/17/word2vec.html\"> The following </a> covers a rigorous derivation of the loss functions used for Word2Vec optimization. Here, we will explain a general approach of how it can be done.\n",
    "\n",
    "1. Likelihood modelling: We want to model the probabilities of our missing words given what we know. In the case of CBOW, we model for $P(w_k | w_1 ... w_c)$ where $w_k$ is the missing word and $w_1 ... w_c$ are the context words. In Skipgram, we model for $P(w_1 .. w_c | w_k)$\n",
    "\n",
    "2. Maximum Likelihood Estimation: we use MLE to identify our loss function. Since we maximize the likelihood, we minimize the negative likelihood.\n",
    "\n",
    "3. Gradient Descent: Once we derive our loss function, we notice that it is not solvable via simple differential optimization. Thus, in order to optimize, we perfom gradient descent.\n",
    "\n",
    "## Some Interesting Misconceptions and Conclusions\n",
    "\n",
    "- Even though we are modelling a prediction task, we do not actually care about the prediction -- only the weights, since they represent dense embeddings. \n",
    "\n",
    "- The meaning of words are determined by their use in the english language. \n",
    "\n",
    "- These embeddings do NOT account for word ordering or context. For instance, the word \"set\" in the sentences \"I *set* the table\" and \"A *set* is an object which encapsulates a collection of objects\" will have the exact same embeddings. \n",
    "\n",
    "- An inbuilt \"feature\" (if you will), is that relationships between words are captured by nature of the geometry of the embedding space. That is, $f(King) - f(Queen) \\approx f(Man) - f(Woman)$\n",
    "\n",
    "- In our toy example, we motivate dense embeddings by suggesting that axes represent certain 'features'. While that is true in an abstract sense, they do not actually represent tangible comprehensible features such as 'size' or 'color'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb942b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69868b7",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "\n",
    "- **Negative Sampling:**  \n",
    "\n",
    "  One of the key things we might notice while going through the math of word2vec is that each probability computation requires computing a softmax across the whole vocabulary. Thus in order to reduce the computational burden, we sample words among the dataset. However, some words are less informative than others. \n",
    "  \n",
    "  For example, consider the task of predicting the word following the phrase \"I ate.\" A common word like \"a,\" which appears frequently throughout the corpus, doesn't offer much distinguishing information. In contrast, a less frequent word such as \"Tacos\" can provide much more valuable context. To emphasize these more informative, rare words during training, we use negative sampling—selecting a set of unlikely words for each positive example.\n",
    "\n",
    "  Negative sampling converts a prediction task into a classification task, instead predicting whether or not the sample generated is a positive or negative sample.\n",
    "\n",
    "  The math behind negative sampling can be studied <a href = \"https://arxiv.org/pdf/1402.3722\"> here </a>\n",
    "\n",
    "\n",
    "- **Adding Context: Contextual embeddings with ELMo:**  \n",
    "\n",
    "  ELMo (Embeddings from Language Models) produces dynamic word representations that vary based on the surrounding context. Rather than assigning a single static vector to each word, ELMo uses deep bidirectional LSTM layers so that for each word at position $t$, the embedding $h_t$ incorporates both past and future context. This allows the same word to have distinct embeddings in different sentences, effectively capturing polysemy.\n",
    "\n",
    "- **Efficient Long Range Dependencies: Attention:**  \n",
    "\n",
    "  The Attention mechanism enables models to dynamically focus on different parts of the input. Given a query $Q$, keys $K$, and values $V$, attention is calculated as:  \n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big) V\n",
    "  $$  \n",
    "  where $d_k$ is the dimension of the key vectors. This method is crucial for capturing long-range dependencies and has been a key component of Transformer-based architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f2e88",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vee-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
